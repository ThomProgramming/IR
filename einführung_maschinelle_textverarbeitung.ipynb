{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Einführung in die maschinelle Sprachverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Vorwort\n",
    "Jupyter Notebooks werden in der Veranstaltung das zentral bereitgestellte Element sein. Die Inhalte teilen sich dabei auf in:\n",
    "* Theorie\n",
    "* Code Beispiele und Umsetzungen der Theorie\n",
    "* Aufgaben\n",
    "\n",
    "Um es Ihnen einfacher zu machen, den Überblick zu behalten, ordnen wir zu Beginn eines neuen Themas/Notebooks zunächst die neuen Inhalte in unsere Pipeline ein. Außerdem wird unter dem Abschnitt _Inhalt_ in Stichwörtern ein kurzer Überblick gegeben.\n",
    "\n",
    "Wir wollen in der Veranstaltung anwendungsorientiert arbeiten, dazu gehört jedoch auch ein Verständnis der Theorie. Deshalb werden die theoretisch erläuterten Inhalte oftmals in Code-Beispielen oder Aufgaben vertieft. Ich bitte Sie daher regelmäßig die Aufgaben zu bearbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Einordnung in der Pipeline\n",
    "\n",
    "Rufen wir uns nochmal unsere Pipeline ins Gedächtnis:\n",
    "\n",
    "![](../resources/pipeline.png)\n",
    "\n",
    "Die Inhalte dieses Notebooks beziehen sich auf den zweiten Schritt unserer Pipeline: das Preprocessing der Textdaten. Da alle Schritte innerhalb solch einer Pipeline aufeinander aufbauen, hängt das Ergebnis späterer Schritte, wie etwa einer Analyse durch statistische Lernverfahren unter anderem auch davon ab, wie gut hier an dieser Stelle gearbeitet wurde. Denn: wenn bei der Datenvorverarbeitung bereits Fehler passieren, wirken die sich unmittelbar auf die Qualität der _gesamten_ Pipeline aus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inhalt\n",
    "\n",
    "* Wichtige Begriffe\n",
    "* Tokenization\n",
    "* Weitere Möglichkeiten der Textverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wichtige Begriffe\n",
    "\n",
    "Die maschinelle Sprachverarbeitung wird auch als Natural Language Processing (NLP) bezeichnet. Hier ist der Fokus ausschließlich auf der Verarbeitung natürlicher Sprache in Textform.\n",
    "\n",
    "In der Veranstaltung werden wir immer wieder auf Grundlagen in diesem Notebook zurückgreifen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "wichtige Begriffe:\n",
    "\n",
    "* **Token/Term:** Kleinste Einheit bei der Verarbeitung von Text. Absichtlich nicht Wort genannt, da frei bestimmbar sein soll, was ein Token ist. Beispielweise besteht _New York_ aus zwei Wörtern, sollte aber sinngemäß als ein einzelnes Token betrachtet werden.\n",
    "* **Dokument:** Ein einzelner Text. Abgrenzung üblicherweise durch das Speichern in einer eigenen Datei gekennzeichnet.\n",
    "* **Korpus/Corpus:** Die gesamte Textsammlung, ergo die Menge aller verfügbaren Dokumente.\n",
    "* **Vokabular:** Die Liste der Tokens, die im Korpus mind. ein Mal vorkommen.\n",
    "* **Tokenizer:** Der Vorgang, bei dem ein Dokument in einzelne Tokens aufgespalten wird. Die Regeln nach denen gehandelt wird können domänenspezifisch sein und sollten an den Korpus angepasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Unter Tokenization wird der Prozess verstanden, bei dem ein Dokument in mehrere Tokens aufgetrennt wird. Da nicht jedes Wort im Text manuell behandelt werden kann, wird dieser Prozess etwa durch Python-Skripte automatisiert. Dafür ist es erforderlich allgemeine Regeln zu definieren.\n",
    "\n",
    "Für viele, wenn nicht sogar fast alle Analysen in dieser Vorlesung ist die Tokenization ein elementarer Bestandteil der Textvorverarbeitung. Die Qualität der Regeln bei der Tokenization hat zudem maßgeblich Einfluss auf die Qualität der Ergebnisse der Analysen, ganze egal ob bei herkömmlichen Algorithmen oder der Anwendung tiefer neuronaler Netze.\n",
    "\n",
    "Die naheliegendste und einfachste Regel für das Trennen in Tokens ist das Trennen an jedem Leerzeichen. Damit erzielt man schonmal ganz gute Ergebnisse, jedoch befinden sich immer noch Sonderzeichen wie Ausrufezeichen oder Fragezeichen im Text. Zudem sollte ein Tokenizer auch immer an die Sprache angepasst sein, beispielsweise gibt es im Spanischen die umgekehrten Fragezeichen ¿ und Ausrufezeichen ¡. Ebenfalls sollten die Regeln für die Tokenization auch der Domäne des Korpus angepasst werden. Oftmals können Sonderzeichen auch bestimmte domänenspezifische Bedeutungen haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktische Funktionen in Python für die String-Verarbeitung:\n",
    "\n",
    "* ``.split(separator)``: Aufspalten des Strings an dem/den Trennzeichen (``separator``). Es wird eine Liste der entstandenen Tokens zurückgegeben.\n",
    "* ``.replace(old, new)``: Ersetzen der alten Zeichenkette (``old``) durch die neue (``new``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich',\n",
       " 'freue',\n",
       " 'mich',\n",
       " 'etwas',\n",
       " 'über',\n",
       " 'maschinelle',\n",
       " 'Sprachverarbeitung',\n",
       " '(NLP)',\n",
       " 'zu',\n",
       " 'lernen.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"Ich freue mich etwas über maschinelle Sprachverarbeitung (NLP) zu lernen.\"\n",
    "document.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Ergebnis\n",
    "\n",
    "Sie sehen: das Dokument konnte bereits in einzelne Tokens aufgeteilt werden. Jedoch was machen wir mit dem Punkt am Ende von letzten Token? Oder den runden Klammern um den Begriff NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### RegEx\n",
    "\n",
    "Mit regulären Ausdrücken (RegEx) können Suchmuster in Zeichenketten definiert werden. Überprüft werden kann damit zum Beispiel, ob unter bestimmten Bedingungen eine Zeichenkette in einer anderen enthalten ist. Im Vergleich zum Beispiel zuvor ist es damit viel einfacher eine Zeichenkette auf alphanumerische Zeichen zu überprüfen. Häufig wird durch reguläre Ausdrücke eine Eingabevalidierung bei Benutzeroberflächen umgesetzt, beispielweise ob eine eingegebene E-Mail-Adresse im richtigen Format ist.\n",
    "\n",
    "Machen Sie sich mit den Regeln und der Syntax von RegEx vertraut, dazu empfiehlt sich die Seite [RegExr](https://regexr.com/). Dort können Sie beliebe reguläre Ausdrücke ausprobieren und erhalten direkt ein Ergebnis. Außerdem ist das Cheat Sheet nützlich.\n",
    "\n",
    "In Python werden ReGex mit dem mitgelieferten Modul ``re`` angewendet. Dort sind folgende für uns relevante Funktionen definiert:\n",
    "\n",
    "* ``findall(pattern, string)``: Gibt eine Liste mit allen gefundenen Treffern zurück.\n",
    "* ``search(pattern, string)``: Gibt die erste gefundene Stelle des ``string`` als Match-Objekt zurück.\n",
    "* ``fullmatch(pattern, string)``: Versucht das ``pattern`` von Beginn des ```string`` an anzuwenden, das heißt, der gesamte String muss das Pattern matchen.\n",
    "* ``split(pattern, string)``: Spaltet den gegebenen ``string`` mit dem ``pattern`` auf, gibt diesen als Liste zurück.\n",
    "* ``sub(pattern, string, txt)``: Ersetzt alle durch das gegebene ``pattern`` erkannte Stellen im ``string`` durch den ``txt``.\n",
    "\n",
    "Häufig enthalten ReGex Sonderzeichen, die in Python Kontrollzeichen innerhalb von Strings sind. Deshalb sollten Reguläre Ausdrücke in Python vor den Anführungszeichen des ReGex-Strings mit einem ``r`` versehen werden, z.B. ``r\"\\.\"``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Aufgaben\n",
    "\n",
    "1. Definieren Sie ein ReGex, dass einen String an Punkten (.) auftrennt.\n",
    "2. Definieren Sie ein ReGex, um alle _zusammenhängenden_ Zahlen aus einem String zu bekommen.\n",
    "3. Schreiben Sie eine Funktion, der ein beliebiger String übergeben werden kann. In der Funktion soll mit _genau_ einem ReGex geprüft, ob der String eine E-Mail im gültigen Format besitzt. Ist dies der Fall wird ``True``, andernfalls ``False`` zurückgegeben. Die Regeln (vereinfacht) für eine gültige E-Mail sollen lauten:\n",
    "  * Nur englische Zeichen, groß/klein sind erlaubt.\n",
    "  * Zahlen sind erlaubt, jedoch nur vor dem @-Zeichen.\n",
    "  * Es muss genau ein ``@``-Zeichen vorkommen.\n",
    "  * Nach dem ``@``-Zeichen folgt die Domain mit der Toplevel-Domain, abgetrennt mit einem Punkt. Diese beliebig langen Zeichenketten enthalten nur englische Zeichen groß oder klein.\n",
    "  * Ansonsten sind keine Sonderzeichen erlaubt.\n",
    "\n",
    "Überprüfen Sie Ihre Lösung auch an den Beispielen auf Gültigkeit, bzw. Ungültigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich freue mich etwas über maschinelle Sprachverarbeitung (NLP) zu lernen',\n",
       " ' Außerdem mag ich auch sehr gerne reguläre Ausdrücke',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aufgabe 1\n",
    "string1 = \"Ich freue mich etwas über maschinelle Sprachverarbeitung (NLP) zu lernen. Außerdem mag ich auch sehr gerne reguläre Ausdrücke.\"\n",
    "\n",
    "re.split(\"\\.\", string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17', '73212']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aufgabe 2\n",
    "string2 = \"Hallo, ich wohne in der Parkstraße 17, 73212 Baumhausen. Student*in\"\n",
    "\n",
    "re.findall(\"\\d+\", string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max.mustermann@web.de is valid\n",
      "max.mustermann@hdm-stuttgart.de is valid\n",
      "mueller@hdm-stuttgart.de is valid\n",
      "jonas.müller@hdm-stuttgart.de is invalid\n",
      "jonas.müller@de is invalid\n",
      "jonas.müller is invalid\n"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3\n",
    "# should be valid\n",
    "emails = [\"max.mustermann@web.de\", \"max.mustermann@hdm-stuttgart.de\", \"mueller@hdm-stuttgart.de\"]\n",
    "# should be invalid\n",
    "emails_invalid = [\"jonas.müller@hdm-stuttgart.de\", \"jonas.müller@de\", \"jonas.müller\"]\n",
    "\n",
    "regex = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "for mail in emails:\n",
    "    if re.match(regex,mail):\n",
    "        print(mail, \"is valid\")\n",
    "    else:\n",
    "        print(mail, \"is invalid\")\n",
    "        \n",
    "for mail in emails_invalid:\n",
    "    if re.match(regex, mail):\n",
    "        print(mail,\"is valid\")\n",
    "    else:\n",
    "        print(mail, \"is invalid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "Schreiben Sie Ihren eigenen Tokenizer. Nutzen Sie dazu die oben vorgestellten Werkzeuge in Python, sodass der Tokenizer folgende Funktionen erfüllt:\n",
    "\n",
    "* Eingabe: eine beliebig lange Zeichenkette, z.B. ein Dokument.\n",
    "* Ausgabe: die Liste der Tokens.\n",
    "* Überlegen Sie welche Regeln für das Splitten in Tokens sinnvoll sind. Auf jeden Fall sollte an Sonderzeichen, aber auch Punkten, Kommata etc. gesplittet werden. Tokens sollten daher nur noch als Alphanumerischen Zeichen bestehen.\n",
    "\n",
    "Testen Sie Ihren Tokenizer zunächst an den Strings aus Aufgabe 1 und 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich',\n",
       " 'freue',\n",
       " 'mich',\n",
       " 'etwas',\n",
       " 'über',\n",
       " 'maschinelle',\n",
       " 'Sprachverarbeitung',\n",
       " 'NLP',\n",
       " 'zu',\n",
       " 'lernen',\n",
       " 'Außerdem',\n",
       " 'mag',\n",
       " 'ich',\n",
       " 'auch',\n",
       " 'sehr',\n",
       " 'gerne',\n",
       " 'reguläre',\n",
       " 'Ausdrücke',\n",
       " 'Hallo',\n",
       " 'ich',\n",
       " 'wohne',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Parkstraße',\n",
       " '17',\n",
       " '73212',\n",
       " 'Baumhausen',\n",
       " 'Student',\n",
       " 'in']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = string1+string2\n",
    "\n",
    "def tokenize(document):\n",
    "    r=re.findall(r\"\\w+\", document)\n",
    "    return r\n",
    "\n",
    "tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Weiterführendes Schritte bei der Textvorverarbeitung: Normalisierung\n",
    "\n",
    "Bei der Analyse von Text mit Python sind Module von großem Nutzen. Wir wollen uns zwei genauer anschauen:\n",
    "* ``nltk`` (Natural Language Tool Kit)\n",
    "* ``spacy``\n",
    "\n",
    "Sie können beide direkt mit pip installieren. NLTK beinhaltet viele grundlegende Werkzeuge für die Verarbeitung von Text, im Folgenden werden einige Beispiele vorgestellt. Spacy dagegen erweitert diese Funktionen durch das Erstellen von NLP-Pipelines bis hin zu bereits fertiger Modelle für die Text-Analyse.\n",
    "\n",
    "Es kann sein, dass einige Beispiele mit nltk beim ersten Mal ausführen nicht direkt funktionieren, da nltk Daten nachladen muss. Folgen Sie der Beschreibung in der Fehlermeldung z.B. ``nltk.download(\"punkt\")``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Groß-/Kleinschreibung\n",
    "\n",
    "Es ist sinnvoll, bei der Analyse von Text die Groß- und Kleinschreibung zu ignorieren. Per Konvention sollten daher alle Tokens zuvor mit der Funktion ``lower()`` in die Kleinschreibung gebracht werden (lower case transformation). In vielen Sprachen hat Großschreibung kaum eine fundamentale Bedeutung wie im Deutschen. Der Informationsverlust der so entstehen kann, etwa wie bei _er macht Pause_ und _die Macht ist stark in ihm_, soll vernachlässigt werden. Der Mehrwert soll dadurch gegeben sein, dass nun auch groß geschriebene Wörter am Satzanfang etc. kleingeschrieben sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Stopfwortfilterung\n",
    "\n",
    "Ein Text in natürlicher Sprache beinhaltet vieler sogenannter Stopfwörter. Damit sind Wörter gemeint, die für die Vollständigkeit und Klang der Sprache wichtig sind, jedoch keine bzw. kaum die eigentliche Informationen tragen. Das Modul ``nltk`` stellt für einige Sprachen vorgefertigte Listen mit sprachspezifischen Stopfwörtern bereit. Im Deutschen handelt es sich dabei vor allem um Attribute, Adverben oder Präpositionen. Schauen Sie sich doch mal die Stopfwörter für die Sprachen Deutsch und Englisch an. In einem späteren Notebook werden wir das Auftreten vcn Stopfwörtern in Texten zudem auch noch einmal genauer betrachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl deutscher Stopfwörter: 232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "print(\"Anzahl deutscher Stopfwörter:\", len(stopwords.words(\"german\")))\n",
    "# deutsche Stopfwörter, alphabetisch sortiert, die ersten 10.\n",
    "stopwords.words(\"german\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Stemming\n",
    "\n",
    "\n",
    "Mit Normalisierung wird in der maschinellen Sprachverarbeitung der Prozess bezeichnet, bei dem einzelne Token nach bestimmten Regeln so gekürzt werden. Stemming ist eine besondere Form der Normalisierung mit dem Ziel, dass nur noch der Wortstamm (engl. stem) übrig bleibt. Im Deutschen ist ein Wort in der Regel folgendermaßen aufgebaut:\n",
    "\n",
    "$$ Präfix + Wortstamm + Suffix $$\n",
    "\n",
    "Präfix und Suffix sind dabei optional. Das Wort _zuvorkommend_ besteht aus Wortstamm und Suffix:\n",
    "\n",
    "$$ zuvorkomm + end $$\n",
    "\n",
    " So würden Tokens im Plural zum Singular oder konjugierte Verben auf ihren Wortstamm zurückgeführt werden. Das Modul ``nltk`` bietet hierfür bereits implementierte Algorithmen für das Stemming. Ein oft eingesetzter Stemmer ist der ``Snowball-Stemmer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zuvorkomm'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "# Beispiel 1\n",
    "stemmer.stem(\"zuvorkommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'erdnuss'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beispiel 2: Plural zu Singular\n",
    "stemmer.stem(\"Erdnüsse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Lemmatisierung\n",
    "\n",
    "Im Gegensatz zum Stemming wird bei der Lemmatisierung das Wort zurück in seine Grundform geführt, man könnte also sagen die Konjugation rückgängig gemacht. Aus dem Wort _ist_ würde demnach das konjugierte Verb zu seinem Infinitiv _sein_ werden. Durch das Definition linguistischer Regeln kann dieser Vorgang automatisiert werden. Jede Sprache braucht dazu jedoch ihre eigenen Sprachregeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e779c7178d1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"de_core_news_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Heute ist ein schöner Tag, auch wenn es regnet, der Himmel grau und die Straßen nass sind.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "text = \"Heute ist ein schöner Tag, auch wenn es regnet, der Himmel grau und die Straßen nass sind.\"\n",
    "doc = nlp(text)\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufgabe\n",
    "\n",
    "Passen Sie ihren zuvor implementierten Tokenizer an, indem Sie ihn um\n",
    "* Lower-Case Transformation\n",
    "* Stopfwortfilterung\n",
    "\n",
    "ergänzen. Überlegen Sie auch welche Reihenfolge der einzelnen Schritte sinnvoll ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freue',\n",
       " 'maschinelle',\n",
       " 'sprachverarbeitung',\n",
       " 'nlp',\n",
       " 'lernen',\n",
       " 'außerdem',\n",
       " 'mag',\n",
       " 'gerne',\n",
       " 'reguläre',\n",
       " 'ausdrücke',\n",
       " 'hallo',\n",
       " 'wohne',\n",
       " 'parkstraße',\n",
       " '17',\n",
       " '73212',\n",
       " 'baumhausen',\n",
       " 'student',\n",
       " 'der',\n",
       " 'über',\n",
       " 'auch',\n",
       " 'sehr',\n",
       " 'zu',\n",
       " 'mich',\n",
       " 'ich',\n",
       " 'in',\n",
       " 'etwas']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_new(document):\n",
    "    stops = set(stopwords.words('german'))\n",
    "    document=document.lower()\n",
    "    r=re.findall(r\"\\w+\", document)\n",
    "    wordsFiltered = []\n",
    "    stopw = []\n",
    "    for w in r:\n",
    "        if w in stops:\n",
    "            stopw.append(w)\n",
    "        else:\n",
    "            wordsFiltered.append(w)\n",
    "    wordsFiltered.extend(list(set(stopw)))\n",
    "    return wordsFiltered\n",
    "\n",
    "tokenize_new(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Der erste Korpus\n",
    "\n",
    "Nachdem grundlegende Werkzeuge für die maschinelle Sprachverarbeitung, wie die Tokenization, erläutert wurden, ist es nun Zeit dies in den Kontext zu setzen. In der Anwendung haben wir es mit Korpora zu tun, also Sammlungen mehrerer Dokumente. Ein Dokument besteht dabei auch nicht aus wenigen Sätzen, sondern aus beliebig vielen.\n",
    "\n",
    "Der erste vorgestellte Korpus dieser Veranstaltung ist eine Dokumentsammlung der deutschsprachigen Wikipedia Artikel über 235 Länder. Sie können den Korpus [hier](https://e-learning.hdm-stuttgart.de/moodle/mod/resource/view.php?id=241791) herunterladen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufgabe\n",
    "\n",
    "1. Verschaffen Sie sich zunächst einen Überblick über die Struktur der Texte des vorliegenden Korpus. Was fällt Ihnen auf? Was sollte man für spätere Analysen im Hinterkopf behalten?\n",
    "\n",
    "2. Wenden Sie nun Ihren Tokenizer auf den Korpus an. Die Ausgabe soll ein Dictionary sein:\n",
    "* Der Key ist der Name des Dokuments.\n",
    "* Der Value eine Liste der Tokens des Dokuments.\n",
    "\n",
    "\n",
    "Tipp: braucht Ihr Computer lange für die Tokenization? Machen Sie aus der Liste der Stopfwörter ein Set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-9c6edbfedf42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdirname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load corpus here\n",
    "import os\n",
    "path = \"\\Studium\\IR\\countries\\countries\"\n",
    "token_dict = {}\n",
    "for file in os.listdir(path):\n",
    "    f=os.path.join(path,file)\n",
    "    if os.path.isfile(f):\n",
    "        t= open(f, encoding = \"utf-8\")\n",
    "        data=t.read()\n",
    "        dict = {\n",
    "            file[:-4]:tokenize_new(data)\n",
    "        }\n",
    "    token_dict.update(dict)\n",
    "#print(token_dict)\n",
    "#funktioniert!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
